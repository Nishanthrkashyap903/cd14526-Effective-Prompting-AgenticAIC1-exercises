{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Lesson Exercise: Organize Your Workspace Space Kick-Off (SOLUTION)\n",
    "\n",
    "In this notebook, we'll explore prompt refinement techniques by guiding an LLM to create and explain a one-hour plan to organize a personal workspace.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Start with a generic prompt\n",
    "2. Add a professional role\n",
    "3. Introduce concrete constraints\n",
    "4. Request step-by-step reasoning\n",
    "5. Reflect on what we've learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using the Vocareum API endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openai.vocareum.com/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# If using OpenAI's API endpoint\n",
    "# client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class OpenAIModels(str, Enum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_41_MINI = \"gpt-4.1-mini\"\n",
    "    GPT_41_NANO = \"gpt-4.1-nano\"\n",
    "\n",
    "\n",
    "MODEL = OpenAIModels.GPT_41_NANO\n",
    "\n",
    "\n",
    "def get_completion(system_prompt, user_prompt, model=MODEL):\n",
    "    \"\"\"\n",
    "    Function to get a completion from the OpenAI API.\n",
    "    Args:\n",
    "        system_prompt: The system prompt\n",
    "        user_prompt: The user prompt\n",
    "        model: The model to use (default is gpt-4.1-mini)\n",
    "    Returns:\n",
    "        The completion text\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    if system_prompt is not None:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            *messages,\n",
    "        ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "def display_responses(*args):\n",
    "    \"\"\"Helper function to display responses as Markdown, horizontally.\"\"\"\n",
    "    markdown_string = \"<table><tr>\"\n",
    "    # Headers\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<th>System Prompt:<br />{arg['system_prompt']}<br /><br />\"\n",
    "        markdown_string += f\"User Prompt:<br />{arg['user_prompt']}</th>\"\n",
    "    markdown_string += \"</tr>\"\n",
    "    # Rows\n",
    "    markdown_string += \"<tr>\"\n",
    "    for arg in args:\n",
    "        markdown_string += f\"<td>Response:<br />{arg['response']}</td>\"\n",
    "    markdown_string += \"</tr></table>\"\n",
    "    display(Markdown(markdown_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generic Prompt\n",
    "\n",
    "First, let's see what the model produces with a basic prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending prompt to OpenAIModels.GPT_41_NANO model...\n",
      "Response received!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<table><tr><th>System Prompt:<br />You are a helpful assistant.<br /><br />User Prompt:<br />Give me a simple plan to declutter and organize my workspace.</th></tr><tr><td>Response:<br />Certainly! Here's a simple step-by-step plan to declutter and organize your workspace:\n",
       "\n",
       "1. Set a Goal:\n",
       "   - Decide what you want to achieve (e.g., a tidy desk, more storage, better organization).\n",
       "2. Clear Everything:\n",
       "   - Remove all items from your desk and surrounding area.\n",
       "3. Sort Items:\n",
       "   - Create piles or categories: Keep, Toss, Donate/Sell.\n",
       "   - Be honest about what you really need.\n",
       "4. Clean the Space:\n",
       "   - Wipe down your desk, shelves, and any other surfaces.\n",
       "5. Organize Essentials:\n",
       "   - Keep only frequently used items within armâ€™s reach.\n",
       "   - Store less-used items in drawers or cabinets.\n",
       "6. Use Storage Solutions:\n",
       "   - Use trays, containers, or organizers for small items.\n",
       "   - Label drawers and containers for easy identification.\n",
       "7. Create a Filing System:\n",
       "   - Organize papers into folders or digital files.\n",
       "8. Maintain Daily:\n",
       "   - Spend 5-10 minutes at the end of each day tidying up.\n",
       "9. Regularly Review:\n",
       "   - Schedule weekly or monthly decluttering sessions to keep things organized.\n",
       "10. Personalize:\n",
       "    - Add a few personal touches like plants or photos to make your space inviting.\n",
       "\n",
       "Would you like a printable checklist or specific product recommendations?</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plain_system_prompt = \"You are a helpful assistant.\"  # A generic system prompt\n",
    "user_prompt = \"Give me a simple plan to declutter and organize my workspace.\"\n",
    "\n",
    "print(f\"Sending prompt to {MODEL} model...\")\n",
    "baseline_response = get_completion(plain_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": plain_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Take a moment to skim the response and note:\n",
    "- What tasks did the model choose?\n",
    "- How did it sequence them?\n",
    "- What level of detail was provided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add a Professional Role\n",
    "\n",
    "Now, let's add a professional role to see how it affects the response.\n",
    "\n",
    "<div style=\"color: red\">NOTE: We will use the same user prompt for these examples. All you need to do is vary the system prompt, which is where one would normally define the role for an LLM.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending prompt with professional role...\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a system prompt starting with \"You are...\" replacing the ***********\n",
    "role_system_prompt = \"You are a certified professional organizer.\"\n",
    "\n",
    "print(\"Sending prompt with professional role...\")\n",
    "role_response = get_completion(role_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": plain_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": baseline_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": role_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": role_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Note the possible differences in:\n",
    "- Terminology used\n",
    "- Level of detail\n",
    "- Suggested tools or techniques\n",
    "- Overall professionalism and expertise reflected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduce Concrete Constraints\n",
    "\n",
    "Let's add specific constraints to see how the model prioritizes tasks. Let's add a time constraint, a budget constraint, and other constraints that are important for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a constraints system prompt replacing the ***********\n",
    "constraints_system_prompt = f\"{role_system_prompt}. I have only 15 minutes, a $20 budget, and limited floor space; I want to keep sentimental items but maximize desk surface.\"\n",
    "\n",
    "print(\"Sending prompt with constraints...\")\n",
    "constraints_response = get_completion(constraints_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Show last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": role_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": role_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": constraints_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": constraints_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Note how the model:\n",
    "- Prioritizes tasks within the given timeframe\n",
    "- Works within the budget\n",
    "- Handles other constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Request Step-by-Step Reasoning\n",
    "\n",
    "Now, let's ask the model to explain its reasoning and provide a final checklist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ask the LLM to explain its reasoning step by step, replacing the ***********\n",
    "reasoning_system_prompt = f\"{constraints_system_prompt}. Explain your reasoning step by step before presenting the final checklist.\"\n",
    "\n",
    "print(\"Sending prompt with reasoning request...\")\n",
    "reasoning_response = get_completion(reasoning_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "# Display the last two prompts and responses\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": constraints_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": constraints_response,\n",
    "    },\n",
    "    {\n",
    "        \"system_prompt\": reasoning_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": reasoning_response,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Examine the chain of thought for:\n",
    "- Clarity of reasoning\n",
    "- Logical progression, or lack thereof\n",
    "- How constraints may have affected decision-making\n",
    "- Additional insights provided by the reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflection & Transfer\n",
    "\n",
    "Let's reflect on what we've learned from this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which prompt tweak changed the output most dramatically and why?\n",
    "\n",
    "Use this cell to jot down your thoughts: Both the constraints and reasoning prompts changed the output significantly in this case. Both of them had a plan at the beginning. The reasoning one expanded this section and actually kept the main response relatively short, and it also added a checklist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List two other situations where the same prompt-refinement pattern could help\n",
    "\n",
    "`TODO: Replace the ***********`\n",
    "1. I want to generate a list of ideas for an article I'd like to write.\n",
    "2. I'm creating a summary of a lengthy technical report for a non-technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Apply what you've learned to a new situation\n",
    "\n",
    "Try crafting a prompt for one of your ideas or even a different organization task (e.g., digital file cleanup, closet overhaul) using the same refinement techniques we explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own prompts here!\n",
    "\n",
    "# TODO: Replace the ***********\n",
    "custom_system_prompt = \"\"\"\n",
    "You are a best-selling writer.\n",
    "\n",
    "In general:\n",
    "- You always explain your overall reasoning before providing the answer to a user's question.\n",
    "- Always conclude with a list of action items when asked for advice.\n",
    "- Do not end with a final question or sentence after this list of action items.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "user_prompt = \"\"\"\n",
    "Please generate a list of 10 ideas for an article about the benefits of meditation\n",
    "that millions of people will read and benefit from. The article is for a technical audience. \n",
    "\"\"\"\n",
    "\n",
    "# Uncomment the lines below to run your custom prompt\n",
    "print(\"Sending custom prompt...\")\n",
    "custom_response = get_completion(custom_system_prompt, user_prompt)\n",
    "print(\"Response received!\\n\")\n",
    "\n",
    "display_responses(\n",
    "    {\n",
    "        \"system_prompt\": custom_system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"response\": custom_response,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, we explored how different prompt refinements affect the output of an LLM:\n",
    "\n",
    "1. **Generic Prompt**: We started with a simple request for a workspace organization plan.\n",
    "2. **Professional Role**: We added a specific role to enhance expertise and authority.\n",
    "3. **Concrete Constraints**: We introduced specific limitations that required prioritization.\n",
    "4. **Step-by-Step Reasoning**: We requested explicit reasoning to understand the model's thought process.\n",
    "\n",
    "These techniques demonstrate how prompt engineering can significantly improve the usefulness and relevance of AI-generated content for specific needs.\n",
    "\n",
    "Excellent Work! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
